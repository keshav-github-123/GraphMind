# ğŸ§  LangGraph Conversational Bot

An **advanced, production-ready conversational AI system** built using **LangGraph**, **LangChain**, **FastAPI**, and **WebSockets**, supporting **tool calling, RAG (Retrieval-Augmented Generation), persistent memory, and MCP (Model Context Protocol) integrations**.

This project demonstrates how to build a **stateful, streaming, multi-tool AI chatbot** with real-world capabilities like document search, knowledge storage, calculators, stock prices, and timezone-aware system utilities.

---

## ğŸš€ Features

### ğŸ” Stateful Conversations (LangGraph)
- Graph-based agent workflow
- Persistent conversation memory using SQLite
- Thread-based chat history with summaries

### ğŸ§° Tool Calling (Local + Remote)
- Arithmetic calculator
- Percentage calculator
- System date & time (timezone-aware)
- Stock price lookup (Alpha Vantage)
- DuckDuckGo web search
- Knowledge base search & save
- **MCP remote tools integration**

### ğŸ“š Retrieval-Augmented Generation (RAG)
- Document embeddings using OpenAI Embeddings
- Vector storage via **ChromaDB**
- Semantic search over uploaded documents & saved notes

### ğŸ”Œ Real-Time Streaming
- WebSocket-based chat
- Token-level streaming responses
- Live tool execution status updates

### ğŸ—‚ï¸ Thread Management
- Automatic conversation summaries
- Thread listing & persistence
- SQLite-backed checkpointing

---

## ğŸ—ï¸ Project Structure

```text
LG_CB/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ graph.py          # LangGraph state machine, tools & agent logic
â”‚   â”œâ”€â”€ main.py           # FastAPI app, WebSocket routes, server startup
â”‚   â”œâ”€â”€ schemas.py        # Pydantic models & request/response schemas
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html        # Simple web-based chat UI
â”œâ”€â”€ uploads/              # User-uploaded documents (PDFs, notes, etc.)
â”œâ”€â”€ vector_db/            # ChromaDB persistent embeddings
â”œâ”€â”€ .env                  # Environment variables (not committed)
â”œâ”€â”€ chatbot.db            # SQLite database (chat memory & summaries)
â””â”€â”€ requirements.txt      # Python dependencies
````

---

## âš™ï¸ Tech Stack

| Layer           | Technology             |
| --------------- | ---------------------- |
| LLM             | OpenAI (GPT-4o-mini)   |
| Agent Framework | LangGraph              |
| Tooling         | LangChain              |
| Backend         | FastAPI                |
| Transport       | WebSockets             |
| Vector DB       | Chroma                 |
| Embeddings      | OpenAI Embeddings      |
| Storage         | SQLite                 |
| MCP             | MultiServer MCP Client |
| Frontend        | HTML + JS              |

---

## ğŸ” Environment Variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_api_key
ALPHA_VANTAGE_API_KEY=your_alpha_vantage_api_key
```

> âš ï¸ Never commit `.env` files to GitHub

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/LG_CB.git
cd LG_CB
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running the Application

### Start Backend Server

```bash
uvicorn backend.main:app --reload
```

* WebSocket endpoint: `ws://localhost:8000/chat`
* API server runs on: `http://localhost:8000`

### Open Frontend

Open `frontend/index.html` in your browser.

---

## ğŸ§  How It Works

1. **User sends a message** via WebSocket
2. Message enters **LangGraph state machine**
3. LLM decides:

   * Respond directly **OR**
   * Call a tool (calculator, RAG, search, MCP, etc.)
4. Tool results are injected back into the graph
5. Response is streamed token-by-token to frontend
6. Conversation state is checkpointed in SQLite
7. First message auto-generates a thread summary

---

## ğŸ§ª Example Tool Calls

* â€œWhatâ€™s todayâ€™s date?â€
* â€œIncrease 5000 by 12%â€
* â€œSearch my documents for Dockerâ€
* â€œSave this note for laterâ€
* â€œGet stock price of AAPLâ€

---

## ğŸ› ï¸ MCP Integration

This project supports **remote MCP servers**:

```python
MultiServerMCPClient({
  "expense": {
    "transport": "sse",
    "url": "https://splendid-gold-dingo.fastmcp.app/mcp"
  }
})
```

MCP tools are dynamically discovered and merged with local tools at runtime.

---

## ğŸ—„ï¸ Persistence & Memory

* **Chat History** â†’ SQLite checkpoints
* **Thread Summaries** â†’ SQLite table
* **Documents & Notes** â†’ ChromaDB
* **Uploads** â†’ Local filesystem

---

## ğŸ“Œ Future Enhancements

* Authentication & user-based threads
* File upload via frontend
* Advanced RAG (chunking, re-ranking)
* Observability (LangSmith / OpenTelemetry)
* Deployment (Docker + AWS / GCP)

---

## ğŸ‘¤ Author

**Keshav Reddy**
Data Analyst | GenAI | LangGraph | MLOps

---

## â­ If you find this useful

Give this repo a â­ and feel free to fork or extend it!
